# Lyft_utils
Classes and functions that help handling Lyft. It is possible to visualize single frames of any camera view and any LIDAR view. As well as retrieving annotation instances and superposing lidar frames to camera views.

To have examples of how to use it, check the jupyter notebook [demos](https://github.com/VictorZuanazzi/Lyft_utils/blob/master/demos.ipynb)

Functinalities include:
* Rendering of complete scene with annotations;
![](https://github.com/VictorZuanazzi/Lyft_utils/blob/master/demo_examples/complete_scene.png)
* Rendering of individual camera point of views;
* Rendering of individual LiDAR sensors;
![](https://github.com/VictorZuanazzi/Lyft_utils/blob/master/demo_examples/pointcloud_topview.png)
* Rendering of the superposition of a camera view and LiDAR;
![](https://github.com/VictorZuanazzi/Lyft_utils/blob/master/demo_examples/superposition.png)
* Retrival of an specific annotation;
* Rendering of multiple frames of all (or any subset) camera views into a video;
* Rendering of multiple frames of any lidar view into a video;
* Rendering of a camera view with the superposed LiDAR into a video;
* Rendering of the bird-eye view of the top lidar (in beta);
![](https://github.com/VictorZuanazzi/Lyft_utils/blob/master/demo_examples/00000.png)
* Rendering of multiple frames of the bird-eye view into a video;

Credits: code modifed from from [Tarun Sriranga Paparaju](https://www.kaggle.com/tarunpaparaju/lyft-competition-understanding-the-data)

